{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16291dde-6dd0-400c-9df4-5469ed7f7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ef3f74-aa55-4826-a137-9a2fc84f6638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/jerry/.cache/huggingface/datasets/text/default-3b1d417f369cc092/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2b5d3309834334922b421181379981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ad146e8cbc4f0fa2b78e75e5dd2e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/jerry/.cache/huggingface/datasets/text/default-3b1d417f369cc092/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dacb49b3904d28917de9273fe0ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": [\"alluneedisattention.txt\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4f5adb-6256-4cf4-a2a0-c4482b719533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 18\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0821408c-0955-4c77-8d52-4e06f564fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7751433e-a05e-4bc5-a849-8122faa56199",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''The dominant sequence transduction models are based on complex recurrent or\n",
    "convolutional neural networks that include an encoder and a decoder. The best\n",
    "performing models also connect the encoder and decoder through an attention\n",
    "mechanism. We propose a new simple network architecture, the Transformer,\n",
    "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
    "entirely. Experiments on two machine translation tasks show these models to\n",
    "be superior in quality while being more parallelizable and requiring significantly\n",
    "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\u0002to-German translation task, improving over the existing best results, including\n",
    "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
    "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
    "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
    "best models from the literature. We show that the Transformer generalizes well to\n",
    "other tasks by applying it successfully to English constituency parsing both with\n",
    "large and limited training data.'''\n",
    "context = context.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "478bc345-cdfe-41f6-9ed7-3413a57348a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d0aa718-5794-43b9-9682-a2ff3308dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(0, len(train), 2):\n",
    "    q = train[i]['text']\n",
    "    a = train[i+1]['text']\n",
    "    q = q.split(':')[-1]\n",
    "    a = a.split(':')[-1]\n",
    "    l.append({'context':context, 'question':q, 'answer':a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "074470b0-1347-4e79-8393-a9a120529526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What is the title of the paper, and who are the authors?',\n",
       "  'answer': 'The title of the paper is \"Attention Is All You Need\". The authors are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What is the objective or research question of the paper?',\n",
       "  'answer': 'The objective of the paper is to propose a new network architecture, the Transformer, based solely on attention mechanisms, which outperforms traditional models in machine translation tasks.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What is the methodology used in the study?',\n",
       "  'answer': 'The methodology used in the study involves proposing a new network architecture, the Transformer, based solely on attention mechanisms. The authors then evaluate the performance of this architecture on two machine translation tasks and compare it to traditional models. They also conduct experiments to analyze the impact of different parameters on the performance of the Transformer.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What are the key findings or results? Summarize the main outcomes or discoveries of the research.',\n",
       "  'answer': '1. The Transformer architecture, based solely on attention mechanisms, outperforms traditional models in machine translation tasks. 2. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 3. The attention mechanism used in the Transformer allows modeling of dependencies without regard to their distance in the input or output sequences.4. Self-attention could yield more interpretable models, and many attention heads exhibit behavior related to the syntactic and semantic structure of sentences.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What are the implications of the findings?',\n",
       "  'answer': 'The findings have several implications for the field of natural language processing and machine translation. The Transformer architecture, based solely on attention mechanisms, provides a new approach to modeling dependencies between input and output sequences that outperforms traditional models. This could lead to improved performance in a wide range of natural language processing tasks. Additionally, the ability of self-attention to yield more interpretable models could help researchers better understand how neural networks process language and improve their ability to diagnose and correct errors. Finally, the increased parallelization enabled by the Transformer architecture could lead to faster training times and more efficient use of computing resources.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What are the limitations of the study?',\n",
       "  'answer': 'Firstly, the study focuses on machine translation tasks and it is unclear how well the Transformer architecture would perform on other natural language processing tasks. Further research is needed to evaluate its performance in other areas such as text classification, sentiment analysis, and question answering. Secondly, the authors only evaluated their model on two specific datasets (WMT 2014 English-to-German and WMT 2014 English-to-French). While these are widely used benchmarks for machine translation, it is unclear how well the model would perform on other datasets or languages. Thirdly, while the authors conducted experiments to analyze the impact of different parameters on the performance of the Transformer, they did not conduct a comprehensive hyperparameter search. It is possible that further tuning could lead to even better performance. Finally, while self-attention allows for more interpretable models, it is still difficult to fully understand how neural networks process language. Further research is needed to develop better methods for interpreting and visualizing neural network models.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'What is the theoretical framework or background literature cited?',\n",
       "  'answer': 'The theoretical framework of the study is based on the use of attention mechanisms in neural network models for natural language processing tasks. The authors cite several previous works that have used attention mechanisms in conjunction with recurrent networks, including Bahdanau et al. (2015) and Luong et al. (2015). They also discuss the limitations of these models, including their reliance on fixed-length context windows and their inability to capture long-range dependencies.The authors propose the Transformer architecture as an alternative approach that relies solely on attention mechanisms to model dependencies between input and output sequences. They cite Vaswani et al. (2017) as a key reference for this architecture, which uses multi-headed self-attention to allow for more parallelization and better modeling of long-range dependencies. Overall, the study builds on a significant body of previous research that has explored the use of attention mechanisms in neural network models for natural language processing tasks. It proposes a novel architecture that represents a major departure from traditional models and provides new insights into how neural networks can be used to model language.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': 'Who is the target audience for this paper?',\n",
       "  'answer': 'The target audience for this paper is likely researchers and practitioners in the field of natural language processing, particularly those interested in machine translation and neural network models. The paper presents a novel architecture, the Transformer, that represents a major advance in the development of neural network models for these tasks. It provides detailed technical descriptions of the architecture and its components, as well as experimental results demonstrating its superior performance compared to traditional models. As such, it is likely to be of interest to researchers and practitioners working on similar problems or interested in developing new neural network architectures for natural language processing tasks.'},\n",
       " {'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       "  'question': \"What are the overall strengths and weaknesses of the paper? Reflect on the paper's merits and limitations.\",\n",
       "  'answer': 'This paper represents a significant advance in the field of natural language processing and provides valuable insights into how neural network models can be used to model language. However, there are some limitations to consider, including its focus on machine translation tasks and its limited evaluation on specific datasets. Further research is needed to fully evaluate its performance in other areas and with other datasets.'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b151d209-7ab2-4768-9175-9b5e655c9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ab262e8-8369-46a2-98ea-d6513d6a35b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answer'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e950eec7-23ce-49ac-8105-a7cbe98baf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036db91540344ebe865c6ac2170cd768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6551065f6e0d45d390affd0532d48fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b9add8c2434cbe910c73ad04b7803f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b49c48f0d430cae1c936e8a38ab5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c724e3-b635-4856-97c1-3b5129ac45ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_dataset = dataset.map(lambda examples: tokenizer(examples[\"question\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fb94034-4506-464a-82c8-06336ec00397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 9\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a33febc3-3b1a-4efd-b6ee-bac0eeb8f590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the title of the paper, and who are the authors?',\n",
       " 'answer': 'The title of the paper is \"Attention Is All You Need\". The authors are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin.',\n",
       " 'input_ids': [101,\n",
       "  1327,\n",
       "  1110,\n",
       "  1103,\n",
       "  1641,\n",
       "  1104,\n",
       "  1103,\n",
       "  2526,\n",
       "  117,\n",
       "  1105,\n",
       "  1150,\n",
       "  1132,\n",
       "  1103,\n",
       "  5752,\n",
       "  136,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1e6275-a25d-4250-a9f4-e8e90853bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "doc_stride = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfc43012-4a77-4d9b-8e90-413ec904023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06b7d151-b28c-4044-979e-65373fe7ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\\x02to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data',\n",
       " 'question': 'What is the title of the paper, and who are the authors?',\n",
       " 'answer': 'The title of the paper is \"Attention Is All You Need\". The authors are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcc5a25f-b3ba-4d78-931c-328d701ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eed9c54f-d84f-42d7-8500-0bd11b7932ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1327, 1110, 1103, 1641, 1104, 1103, 2526, 117, 1105, 1150, 1132, 1103, 5752, 136, 102, 1109, 7065, 4954, 14715, 11243, 3584, 1132, 1359, 1113, 2703, 1231, 21754, 1137, 14255, 6005, 18404, 1348, 18250, 6379, 1115, 1511, 1126, 4035, 13775, 1197, 1105, 170, 1260, 13775, 1197, 119, 1109, 1436, 4072, 3584, 1145, 7543, 1103, 4035, 13775, 1197, 1105, 1260, 13775, 1197, 1194, 1126, 2209, 6978, 119, 1284, 17794, 170, 1207, 3014, 2443, 4220, 117, 1103, 13809, 23763, 117, 1359, 9308, 1113, 2209, 10748, 117, 4267, 20080, 5026, 1158, 1114, 1231, 10182, 21629, 1105, 14255, 6005, 18404, 1116, 3665, 119, 28009, 1116, 1113, 1160, 3395, 5179, 8249, 1437, 1292, 3584, 1106, 1129, 7298, 1107, 3068, 1229, 1217, 1167, 5504, 23228, 2165, 1105, 8753, 5409, 1750, 1159, 1106, 2669, 119, 3458, 2235, 5515, 1116, 1743, 119, 125, 139, 17516, 2591, 1113, 1103, 160, 13910, 1387, 1483, 2430, 118, 1528, 5179, 4579, 117, 9248, 1166, 1103, 3685, 1436, 2686, 117, 1259, 24957, 117, 1118, 1166, 123, 139, 17516, 2591, 119, 1212, 1103, 160, 13910, 1387, 1483, 118, 1106, 118, 1497, 5179, 4579, 117, 1412, 2235, 23497, 170, 1207, 1423, 118, 2235, 1352, 118, 1104, 118, 1103, 118, 1893, 139, 17516, 2591, 2794, 1104, 3746, 119, 129, 1170, 2013, 1111, 124, 119, 126, 1552, 1113, 2022, 15175, 2591, 1116, 117, 170, 1353, 13394, 1104, 1103, 2013, 4692, 1104, 1103, 1436, 3584, 1121, 1103, 3783, 119, 1284, 1437, 1115, 1103, 13809, 23763, 1704, 9534, 1218, 1106, 1168, 8249, 1118, 11892, 1122, 4358, 1106, 1483, 5269, 14247, 4253, 1241, 1114, 1415, 1105, 2609, 2013, 2233, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56429c21-2b78-4268-9f32-bf818029ee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What is the title of the paper, and who are the authors? [SEP] The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28. 4 BLEU on the WMT 2014 Englishto - German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English - to - French translation task, our model establishes a new single - model state - of - the - art BLEU score of 41. 8 after training for 3. 5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c1eed63-55e4-4cad-b789-a06908581775",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example2 = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f7d1052-ab9c-493f-abd2-f41308a901ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1327, 1110, 1103, 1641, 1104, 1103, 2526, 117, 1105, 1150, 1132, 1103, 5752, 136, 102, 1109, 7065, 4954, 14715, 11243, 3584, 1132, 1359, 1113, 2703, 1231, 21754, 1137, 14255, 6005, 18404, 1348, 18250, 6379, 1115, 1511, 1126, 4035, 13775, 1197, 1105, 170, 1260, 13775, 1197, 119, 1109, 1436, 4072, 3584, 1145, 7543, 1103, 4035, 13775, 1197, 1105, 1260, 13775, 1197, 1194, 1126, 2209, 6978, 119, 1284, 17794, 170, 1207, 3014, 2443, 4220, 117, 1103, 13809, 23763, 117, 1359, 9308, 1113, 2209, 10748, 117, 4267, 20080, 5026, 1158, 1114, 1231, 10182, 21629, 1105, 14255, 6005, 18404, 1116, 3665, 119, 28009, 1116, 1113, 1160, 3395, 5179, 8249, 1437, 1292, 3584, 1106, 1129, 7298, 1107, 3068, 1229, 1217, 1167, 5504, 23228, 2165, 1105, 8753, 5409, 1750, 1159, 1106, 2669, 119, 3458, 2235, 5515, 1116, 1743, 119, 125, 139, 17516, 2591, 1113, 1103, 160, 13910, 1387, 1483, 2430, 118, 1528, 5179, 4579, 117, 9248, 1166, 1103, 3685, 1436, 2686, 117, 1259, 24957, 117, 1118, 1166, 123, 139, 17516, 2591, 119, 1212, 1103, 160, 13910, 1387, 1483, 118, 1106, 118, 1497, 5179, 4579, 117, 1412, 2235, 23497, 170, 1207, 1423, 118, 2235, 1352, 118, 1104, 118, 1103, 118, 1893, 139, 17516, 2591, 2794, 1104, 3746, 119, 129, 1170, 2013, 1111, 124, 119, 126, 1552, 1113, 2022, 15175, 2591, 1116, 117, 170, 1353, 13394, 1104, 1103, 2013, 4692, 1104, 1103, 1436, 3584, 1121, 1103, 3783, 119, 1284, 1437, 1115, 1103, 13809, 23763, 1704, 9534, 1218, 1106, 1168, 8249, 1118, 11892, 1122, 4358, 1106, 1483, 5269, 14247, 4253, 1241, 1114, 1415, 1105, 2609, 2013, 2233, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 11), (12, 17), (18, 20), (21, 24), (25, 30), (30, 31), (32, 35), (36, 39), (40, 43), (44, 47), (48, 55), (55, 56), (0, 0), (0, 3), (4, 12), (13, 21), (22, 27), (27, 34), (35, 41), (42, 45), (46, 51), (52, 54), (55, 62), (63, 65), (65, 72), (73, 75), (76, 79), (79, 81), (81, 87), (87, 89), (90, 96), (97, 105), (106, 110), (111, 118), (119, 121), (122, 124), (124, 128), (128, 129), (130, 133), (134, 135), (136, 138), (138, 142), (142, 143), (143, 144), (145, 148), (149, 153), (154, 164), (165, 171), (172, 176), (177, 184), (185, 188), (189, 191), (191, 195), (195, 196), (197, 200), (201, 203), (203, 207), (207, 208), (209, 216), (217, 219), (220, 229), (230, 239), (239, 240), (241, 243), (244, 251), (252, 253), (254, 257), (258, 264), (265, 272), (273, 285), (285, 286), (287, 290), (291, 296), (296, 302), (302, 303), (304, 309), (310, 316), (317, 319), (320, 329), (330, 340), (340, 341), (342, 344), (344, 346), (346, 349), (349, 352), (353, 357), (358, 360), (360, 362), (362, 368), (369, 372), (373, 376), (376, 378), (378, 384), (384, 385), (386, 394), (394, 395), (396, 406), (406, 407), (408, 410), (411, 414), (415, 422), (423, 434), (435, 440), (441, 445), (446, 451), (452, 458), (459, 461), (462, 464), (465, 473), (474, 476), (477, 484), (485, 490), (491, 496), (497, 501), (502, 510), (510, 513), (513, 516), (517, 520), (521, 530), (531, 544), (545, 549), (550, 554), (555, 557), (558, 563), (563, 564), (565, 568), (569, 574), (575, 582), (582, 583), (584, 586), (586, 587), (587, 588), (589, 590), (590, 592), (592, 593), (594, 596), (597, 600), (601, 602), (602, 604), (605, 609), (610, 617), (618, 620), (620, 621), (621, 627), (628, 639), (640, 644), (644, 645), (646, 655), (656, 660), (661, 664), (665, 673), (674, 678), (679, 686), (686, 687), (688, 697), (698, 707), (707, 708), (709, 711), (712, 716), (717, 718), (719, 720), (720, 722), (722, 723), (723, 724), (725, 727), (728, 731), (732, 733), (733, 735), (736, 740), (741, 748), (748, 749), (749, 751), (751, 752), (752, 758), (759, 770), (771, 775), (775, 776), (777, 780), (781, 786), (787, 798), (799, 800), (801, 804), (805, 811), (811, 812), (812, 817), (818, 823), (823, 824), (824, 826), (826, 827), (827, 830), (830, 831), (831, 834), (835, 836), (836, 838), (838, 839), (840, 845), (846, 848), (849, 851), (851, 852), (852, 853), (854, 859), (860, 868), (869, 872), (873, 874), (874, 875), (875, 876), (877, 881), (882, 884), (885, 890), (891, 893), (893, 894), (894, 895), (895, 896), (897, 898), (899, 904), (905, 913), (914, 916), (917, 920), (921, 929), (930, 935), (936, 938), (939, 942), (943, 947), (948, 954), (955, 959), (960, 963), (964, 974), (974, 975), (976, 978), (979, 983), (984, 988), (989, 992), (993, 998), (998, 1004), (1005, 1012), (1012, 1016), (1017, 1021), (1022, 1024), (1025, 1030), (1031, 1036), (1037, 1039), (1040, 1048), (1049, 1051), (1052, 1064), (1065, 1067), (1068, 1075), (1076, 1088), (1089, 1092), (1092, 1096), (1097, 1101), (1102, 1106), (1107, 1112), (1113, 1116), (1117, 1124), (1125, 1133), (1134, 1138), (0, 0)]], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba2c5524-80a7-4842-ad20-c0f997891f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = tokenized_example2['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9e0b2ed-1ef0-4a10-9f28-fef182817a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(offsets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2aea2bc5-a7fb-4290-a86e-9cc427a9de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_example2['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01129d6-50a2-4495-8668-5827c8311943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908db51e-8ad2-48e7-babb-c92cb2d9848a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b3383-d05e-494e-a640-c711f4780379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b05e6-33b4-4eac-8ba5-23395d453596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f1d88-b1e7-430d-8a3c-5fad1a5553dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff57bc0-d22a-4fad-b278-cd18b6856478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739a86a-d6b0-4365-90b2-82a1c7a2a202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64474051-5380-4cda-b81c-df5807f76c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfdb04-989f-4f38-ad9a-ea3819a14cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf860fd-733f-41ca-99da-61dbe65f32c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "# max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(examples['question'], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"answer\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
