{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b9d95-fe07-4929-9e13-e0281d3450ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading model\n",
    "def load_model(model_path):\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        'decapoda-research/llama-7b-hf',\n",
    "        load_in_8bit=True,  # 8-bit to save VRAM\n",
    "        device_map=device_map,\n",
    "        cache_dir=OUTPUT_DIR\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760f537-500b-47c6-b8d7-392b6baede7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# use local\n",
    "use_loacl = True #set equal to true if alreday finetuned\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 1234\n",
    "transformers.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Fit into Kaggle T4*2\n",
    "MICRO_BATCH_SIZE = 4\n",
    "BATCH_SIZE = 128\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 1  # One epoch takes ~6 hours, and 2 epochs may exceed Kaggle 12-hour limit \n",
    "LEARNING_RATE = 2e-5  # Following stanford_alpaca\n",
    "CUTOFF_LEN = 256  # 256 accounts for about 96% of the data. Shorter input, faster training/less VRAM\n",
    "LORA_R = 8  # Some LoRA parameters\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "VAL_SET_SIZE = 2000\n",
    "TARGET_MODULES = [\n",
    "    'q_proj',\n",
    "    'v_prol',\n",
    "]\n",
    "#DATA_PATH = 'final_dataset.json'\n",
    "OUTPUT_DIR = './working/wealth-alpaca'  # Save the model in Kaggle output dir.\n",
    "\n",
    "# DDP setting\n",
    "device_map = 'auto'\n",
    "world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
    "ddp = (world_size != 1)  # If more than one GPU, then DDP\n",
    "if ddp:\n",
    "    device_map = {'': int(os.environ.get('LOCAL_RANK') or 0)}\n",
    "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    'decapoda-research/llama-7b-hf', add_eos_token=True\n",
    ")\n",
    "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "\n",
    "# Read LLaMA model\n",
    "if(use_local):\n",
    "    model = load_model(OUTPUT_DIR)\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "    'decapoda-research/llama-7b-hf',\n",
    "    load_in_8bit=True,  # 8-bit to save VRAM\n",
    "    device_map=device_map,\n",
    "    )\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# LoRA config.\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "data = load_dataset('gbharti/wealth-alpaca_lora')\n",
    "data = data.shuffle(seed=RANDOM_SEED)  # Shuffle dataset here\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: str: Input text\n",
    "    \"\"\"\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input']:\n",
    "        text = 'Below is an instruction that describes a task, paired with an input that provides' \\\n",
    "               ' further context. Write a response that appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Input:\\n{data_point[\"input\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
    "        return text\n",
    "\n",
    "    # Without\n",
    "    else:\n",
    "        text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "        text += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        text += f'### Response:\\n{data_point[\"output\"]}'\n",
    "        return text\n",
    "\n",
    "\n",
    "def tokenize(prompt):\n",
    "    \"\"\"Tokenise the input\n",
    "\n",
    "    :param prompt: str: Input text\n",
    "    :return: dict: {'tokenised input text': list, 'mask': list}\n",
    "    \"\"\"\n",
    "    result = tokenizer(prompt, truncation=True, max_length=CUTOFF_LEN + 1, padding='max_length')\n",
    "    return {\n",
    "        'input_ids': result['input_ids'][:-1],\n",
    "        'attention_mask': result['attention_mask'][:-1],\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"This function masks out the labels for the input, so that our loss is computed only on the\n",
    "    response.\"\"\"\n",
    "    if data_point['input']:\n",
    "        user_prompt = 'Below is an instruction that describes a task, paired with an input that ' \\\n",
    "                      'provides further context. Write a response that appropriately completes ' \\\n",
    "                      'the request.\\n\\n'\n",
    "        user_prompt += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        user_prompt += f'### Input:\\n{data_point[\"input\"]}\\n\\n'\n",
    "        user_prompt += f'### Response:\\n'\n",
    "    else:\n",
    "        user_prompt = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "                      'appropriately completes the request.'\n",
    "        user_prompt += f'### Instruction:\\n{data_point[\"instruction\"]}\\n\\n'\n",
    "        user_prompt += f'### Response:\\n'\n",
    "\n",
    "    # Count the length of prompt tokens\n",
    "    len_user_prompt_tokens = len(tokenizer(user_prompt,\n",
    "                                           truncation=True,\n",
    "                                           max_length=CUTOFF_LEN + 1,\n",
    "                                           padding='max_length')['input_ids'])\n",
    "    len_user_prompt_tokens -= 1  # Minus 1 (one) for eos token\n",
    "\n",
    "    # Tokenise the input, both prompt and output\n",
    "    full_tokens = tokenizer(\n",
    "        user_prompt + data_point['output'],\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding='max_length',\n",
    "    )['input_ids'][:-1]\n",
    "    return {\n",
    "        'input_ids': full_tokens,\n",
    "        'labels': [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n",
    "        'attention_mask': [1] * (len(full_tokens)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Train/val split\n",
    "if VAL_SET_SIZE > 0:\n",
    "    train_val = data['train'].train_test_split(\n",
    "        test_size=VAL_SET_SIZE, shuffle=False, seed=RANDOM_SEED\n",
    "    )\n",
    "    train_data = train_val['train'].map(generate_and_tokenize_prompt)\n",
    "    val_data = train_val['test'].map(generate_and_tokenize_prompt)\n",
    "else:\n",
    "    train_data = data['train'].map(generate_and_tokenize_prompt)\n",
    "    val_data = None\n",
    "\n",
    "# HuggingFace Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        seed=RANDOM_SEED,  # Reproducibility\n",
    "        data_seed=RANDOM_SEED,\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy='steps' if VAL_SET_SIZE > 0 else 'no',\n",
    "        save_strategy='steps',\n",
    "        save_steps=50,\n",
    "        eval_steps=50 if VAL_SET_SIZE > 0 else None,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n",
    "        ddp_find_unused_parameters=False if ddp else None,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# PEFT setup\n",
    "old_state_dict = model.state_dict\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n",
    "\n",
    "# # Use the latest PyTorch 2.0 if possible\n",
    "# if torch.__version__ >= '2' and sys.platform != 'win32':\n",
    "#     model = torch.compile(model)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "wandb.finish()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64b419-ca25-4c56-b7ea-32edc65dfab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR+'/checkpoint-300',\n",
    "    load_in_8bit=True,  # 8-bit to save VRAM\n",
    "    device_map=device_map,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d2221-2c51-44a8-905e-a01070685c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9525e24-fd5a-4777-8b70-3ba05a3c5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question):\n",
    "    return f\"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    Instruction: {question}\n",
    "    Result:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c8ab4-10f4-40a9-adcc-256cd2669ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541f0ee-9e6f-4c19-b4db-a364b85b6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input=None):\n",
    "    start = time.time()\n",
    "    outs = []\n",
    "    prompt = generate_prompt(input)\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(temperature=0.2, top_p=0.75, num_beams=4),\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    \n",
    "    for seq in generation_output.sequences:\n",
    "        output = tokenizer.decode(seq)\n",
    "        outs.append(output)\n",
    "        print(output.split('Result:')[1])\n",
    "        print()\n",
    "    end = time.time()\n",
    "    print(f\"runtime: {end-start}\")\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79d9c7-3aab-481c-9189-396d29895472",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = generate('Give me the financial news on July 30th.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8396a2e-4a7d-40df-b347-6eec9c77ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = generate('Give me the financial news on July 30th, 2023.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695870ea-6c7d-4a55-96a6-842a78f8f529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi",
   "language": "python",
   "name": "agi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
